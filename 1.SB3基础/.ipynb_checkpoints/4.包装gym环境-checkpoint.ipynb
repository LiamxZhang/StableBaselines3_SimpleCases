{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9555506   0.29482716  0.889765  ]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "#自定义一个Wrapper\n",
    "class Pendulum(gym.Wrapper):\n",
    "\n",
    "    def __init__(self):\n",
    "        env = gym.make('Pendulum-v1',render_mode=\"human\")\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        state, info = self.env.reset()\n",
    "        return state, info\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "print(Pendulum().reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>\n",
      "0 [ 0.8690126  -0.49478996  0.25852925] [0.38116938] -0.2712292164795527\n",
      "20 [-0.84894484 -0.5284814   7.14058   ] [-0.36051726] -9.539666244964971\n",
      "40 [ 0.47867265 -0.87799346 -4.017311  ] [-0.1569533] -3.9156727454337834\n",
      "60 [ 0.50958234 -0.8604219   0.9194922 ] [1.3635069] -1.0492464685213325\n",
      "80 [-0.5145037  -0.85748816  6.737064  ] [0.5846952] -7.069341633539481\n",
      "100 [ 0.8777114   0.47918963 -1.3289554 ] [0.04540587] -0.2966280328836356\n",
      "120 [0.62123334 0.78362566 3.0971074 ] [-1.5702893] -2.3434320261914268\n",
      "140 [-0.09331109  0.995637   -4.415158  ] [1.4396266] -3.41896588222713\n",
      "160 [0.22346951 0.97471094 4.3902297 ] [0.6518642] -5.304709052746031\n",
      "180 [ 0.3853862  -0.92275536  3.92823   ] [-0.12849551] -2.063710473386545\n",
      "200 [ 0.7604966 -0.6493419 -3.059499 ] [1.1502334] -1.9060460989891752\n"
     ]
    }
   ],
   "source": [
    "#测试一个环境\n",
    "def test(env, wrap_action_in_list=False):\n",
    "    print(env)\n",
    "    try:\n",
    "        state = env.reset()\n",
    "    except:\n",
    "        state, _ = env.reset()\n",
    "    over = False\n",
    "    step = 0\n",
    "\n",
    "    while not over:\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        if wrap_action_in_list:\n",
    "            action = [action]\n",
    "        try:\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "        except:\n",
    "            next_state, reward, over, _, _ = env.step(action)\n",
    "        if step % 20 == 0:\n",
    "            print(step, state, action, reward)\n",
    "\n",
    "        if step > 200:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "        step += 1\n",
    "\n",
    "\n",
    "test(Pendulum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eb2U4_K6SNUx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StepLimitWrapper<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 [-0.17543022 -0.9844919  -0.9861332 ] [-0.37350124] -3.14988032699306\n",
      "20 [-0.19588898  0.9806261   1.3804784 ] [1.7438195] -3.319296344256698\n",
      "40 [-0.5027321 -0.8644423 -2.720407 ] [0.20915733] -5.139832699340967\n",
      "60 [-0.8646059   0.50245064  4.2143946 ] [0.23335248] -8.61523780681135\n",
      "80 [-0.99498296  0.10004468 -4.2221828 ] [1.9384729] -11.036434684496214\n"
     ]
    }
   ],
   "source": [
    "#修改最大步数\n",
    "class StepLimitWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "        #修改done字段\n",
    "        if self.current_step >= 100:\n",
    "            done = True\n",
    "\n",
    "        return state, reward, done, info\n",
    "\n",
    "\n",
    "test(StepLimitWrapper(Pendulum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F5E6kZfzW8vy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NormalizeActionWrapper<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n",
      "0 [-0.54957944  0.83544147 -0.09078036] [-0.16019736] -4.763413029567849\n",
      "20 [-0.9533661   0.30181634  1.9025816 ] [-0.49939758] -9.000762938304984\n",
      "40 [-0.47326204 -0.8809217   2.1182206 ] [-0.24065462] -4.170255549783669\n",
      "60 [-0.42249337 -0.906366   -1.3803447 ] [-0.5824233] -4.989840889349022\n",
      "80 [-0.9957207  -0.09241352 -3.4013486 ] [0.37694225] -10.529846907911079\n",
      "100 [-0.81958973  0.5729508  -4.77027   ] [0.1187015] -7.218297211810192\n",
      "120 [-0.1382315   0.99039996 -6.0278034 ] [-0.92172736] -5.145344530552183\n",
      "140 [ 0.4695109   0.88292664 -4.2409635 ] [0.88921374] -1.93999787628309\n",
      "160 [-0.02428142 -0.99970514 -3.6422586 ] [-0.14378788] -5.2680628703789365\n",
      "180 [-0.07344457 -0.9972993  -3.2208362 ] [0.8033563] -4.74368605110887\n",
      "200 [-0.4509822  -0.89253294 -2.6284041 ] [-0.42209548] -6.056971459022459\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#修改动作空间\n",
    "class NormalizeActionWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        #获取动作空间\n",
    "        action_space = env.action_space\n",
    "\n",
    "        #动作空间必须是连续值\n",
    "        assert isinstance(action_space, gym.spaces.Box)\n",
    "\n",
    "        #重新定义动作空间,在正负一之间的连续值\n",
    "        #这里其实只影响env.action_space.sample的返回结果\n",
    "        #实际在计算时,还是正负2之间计算的\n",
    "        env.action_space = gym.spaces.Box(low=-1,\n",
    "                                          high=1,\n",
    "                                          shape=action_space.shape,\n",
    "                                          dtype=np.float32)\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        #重新缩放动作的值域\n",
    "        action = action * 2.0\n",
    "\n",
    "        if action > 2.0:\n",
    "            action = 2.0\n",
    "\n",
    "        if action < -2.0:\n",
    "            action = -2.0\n",
    "\n",
    "        return self.env.step(action)\n",
    "\n",
    "\n",
    "test(NormalizeActionWrapper(Pendulum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBlS9YxYSpJn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<StateStepWrapper<Pendulum<TimeLimit<OrderEnforcing<PassiveEnvChecker<PendulumEnv<Pendulum-v1>>>>>>>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(env, wrap_action_in_list)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 5\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mStateStepWrapper.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m         state_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_current \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate([state, [state_step]])\n\u001b[0;32m---> 45\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStateStepWrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPendulum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(env, wrap_action_in_list)\u001b[0m\n\u001b[1;32m      5\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     state, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mStateStepWrapper.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import TimeLimit\n",
    "\n",
    "\n",
    "#修改状态\n",
    "class StateStepWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "\n",
    "        #状态空间必须是连续值\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        #增加一个新状态字段\n",
    "        low = np.concatenate([env.observation_space.low, [0.0]])\n",
    "        high = np.concatenate([env.observation_space.high, [1.0]])\n",
    "\n",
    "        env.observation_space = gym.spaces.Box(low=low,\n",
    "                                               high=high,\n",
    "                                               dtype=np.float32)\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.step_current = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_current = 0\n",
    "        return np.concatenate([self.env.reset()[0], [0.0]])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_current += 1\n",
    "        state, reward, done, _, info = self.env.step(action)\n",
    "\n",
    "        #根据step_max修改done\n",
    "        if self.step_current >= 100:\n",
    "            done = True\n",
    "\n",
    "        return self.get_state(state), reward, done, info\n",
    "\n",
    "    def get_state(self, state):\n",
    "        #添加一个新的state字段\n",
    "        state_step = self.step_current / 100\n",
    "\n",
    "        return np.concatenate([state, [state_step]])\n",
    "\n",
    "\n",
    "test(StateStepWrapper(Pendulum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cxnE5bdaQ_3"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "#使用Monitor Wrapper,会在训练的过程中输出rollout/ep_len_mean和rollout/ep_rew_mean,就是增加些日志\n",
    "#gym升级到0.26以后失效了,可能是因为使用了自定义的wapper\n",
    "env = DummyVecEnv([lambda: Monitor(Pendulum().env)])\n",
    "\n",
    "A2C('MlpPolicy', env, verbose=1).learn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuIcbfv3g9dd"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "\n",
    "#VecNormalize,他会对state和reward进行Normalize\n",
    "#Pendulum = lambda: gym.make('Pendulum-v1',render_mode=\"human\")\n",
    "env = DummyVecEnv([Pendulum])\n",
    "env = VecNormalize(env)\n",
    "\n",
    "#state = env.reset()\n",
    "#action = env.action_space.sample()\n",
    "#print(env.step([action]))\n",
    "\n",
    "test(env, wrap_action_in_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "2_gym_wrappers_saving_loading.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
